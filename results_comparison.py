#!/usr/bin/env python3
"""
Results Comparison Script for AI.SEE Assessment
This script compares the results between PyTorch and ONNX model inference.
"""

import os
import json
import numpy as np
import matplotlib.pyplot as plt
import cv2
from pathlib import Path
import pandas as pd


class ResultsComparison:
    """Class to compare PyTorch and ONNX inference results."""
    
    def __init__(self, output_dir="output"):
        """
        Initialize the results comparison.
        
        Args:
            output_dir (str): Directory containing the results
        """
        self.output_dir = Path(output_dir)
        self.pytorch_results_file = self.output_dir / "pytorch_results.txt"
        self.onnx_results_file = self.output_dir / "onnx_results.txt"
        self.pytorch_image = self.output_dir / "pytorch_detections.png"
        self.onnx_image = self.output_dir / "onnx_detections.png"
        
    def load_results(self):
        """Load results from both PyTorch and ONNX inference."""
        print("Loading inference results...")
        
        pytorch_detections = self._parse_results_file(self.pytorch_results_file, "PyTorch")
        onnx_detections = self._parse_results_file(self.onnx_results_file, "ONNX")
        
        return pytorch_detections, onnx_detections
    
    def _parse_results_file(self, results_file, model_type):
        """Parse results from a text file."""
        if not results_file.exists():
            print(f"‚ùå {model_type} results file not found: {results_file}")
            return []
        
        detections = []
        
        try:
            with open(results_file, 'r') as f:
                lines = f.readlines()
            
            # Find the detection details section
            in_detection_section = False
            
            for line in lines:
                line = line.strip()
                
                if "DETECTION DETAILS:" in line:
                    in_detection_section = True
                    continue
                
                if in_detection_section and line and not line.startswith("-") and not line.startswith("Class"):
                    # Parse detection line
                    parts = line.split()
                    if len(parts) >= 5:
                        try:
                            class_name = parts[0]
                            confidence = float(parts[1])
                            bbox_str = parts[2:6]  # x1, y1, x2, y2
                            bbox = [float(x.strip('[],')) for x in bbox_str]
                            area = float(parts[6])
                            
                            detections.append({
                                'class_name': class_name,
                                'confidence': confidence,
                                'bbox': bbox,
                                'area': area
                            })
                        except (ValueError, IndexError) as e:
                            print(f"Warning: Could not parse line: {line}")
                            continue
            
            print(f"‚úÖ Loaded {len(detections)} detections from {model_type}")
            return detections
            
        except Exception as e:
            print(f"‚ùå Error parsing {model_type} results: {e}")
            return []
    
    def compare_detections(self, pytorch_detections, onnx_detections):
        """Compare detections between PyTorch and ONNX models."""
        print("\n" + "="*60)
        print("DETECTION COMPARISON ANALYSIS")
        print("="*60)
        
        # Basic statistics
        print(f"\nDetection Counts:")
        print(f"  PyTorch: {len(pytorch_detections)} detections")
        print(f"  ONNX:    {len(onnx_detections)} detections")
        print(f"  Difference: {abs(len(pytorch_detections) - len(onnx_detections))}")
        
        # Class distribution comparison
        pytorch_classes = [d['class_name'] for d in pytorch_detections]
        onnx_classes = [d['class_name'] for d in onnx_detections]
        
        print(f"\nClass Distribution:")
        print(f"  PyTorch classes: {set(pytorch_classes)}")
        print(f"  ONNX classes:    {set(onnx_classes)}")
        
        # Confidence comparison
        if pytorch_detections and onnx_detections:
            pytorch_confidences = [d['confidence'] for d in pytorch_detections]
            onnx_confidences = [d['confidence'] for d in onnx_detections]
            
            print(f"\nConfidence Statistics:")
            print(f"  PyTorch - Min: {min(pytorch_confidences):.4f}, Max: {max(pytorch_confidences):.4f}, Mean: {np.mean(pytorch_confidences):.4f}")
            print(f"  ONNX     - Min: {min(onnx_confidences):.4f}, Max: {max(onnx_confidences):.4f}, Mean: {np.mean(onnx_confidences):.4f}")
        
        # Detailed comparison for each detection
        print(f"\nDetailed Detection Comparison:")
        print("-" * 80)
        print(f"{'Model':<10} {'Class':<15} {'Confidence':<12} {'BBox (x1,y1,x2,y2)':<25} {'Area':<10}")
        print("-" * 80)
        
        # Show PyTorch detections
        for i, detection in enumerate(pytorch_detections):
            print(f"{'PyTorch':<10} {detection['class_name']:<15} {detection['confidence']:<12.4f} "
                  f"{str(detection['bbox']):<25} {detection['area']:<10.1f}")
        
        # Show ONNX detections
        for i, detection in enumerate(onnx_detections):
            print(f"{'ONNX':<10} {detection['class_name']:<15} {detection['confidence']:<12.4f} "
                  f"{str(detection['bbox']):<25} {detection['area']:<10.1f}")
        
        return self._calculate_similarity_metrics(pytorch_detections, onnx_detections)
    
    def _calculate_similarity_metrics(self, pytorch_detections, onnx_detections):
        """Calculate similarity metrics between PyTorch and ONNX results."""
        print(f"\nSimilarity Analysis:")
        print("-" * 40)
        
        # IoU-based matching (simplified)
        matches = 0
        total_detections = max(len(pytorch_detections), len(onnx_detections))
        
        if total_detections == 0:
            print("No detections to compare")
            return {"match_rate": 0.0, "avg_confidence_diff": 0.0}
        
        # Simple matching based on class and approximate bbox overlap
        for pytorch_det in pytorch_detections:
            for onnx_det in onnx_detections:
                if pytorch_det['class_name'] == onnx_det['class_name']:
                    # Calculate IoU
                    iou = self._calculate_iou(pytorch_det['bbox'], onnx_det['bbox'])
                    if iou > 0.5:  # Threshold for matching
                        matches += 1
                        break
        
        match_rate = matches / total_detections if total_detections > 0 else 0.0
        
        # Calculate average confidence difference
        if pytorch_detections and onnx_detections:
            avg_pytorch_conf = np.mean([d['confidence'] for d in pytorch_detections])
            avg_onnx_conf = np.mean([d['confidence'] for d in onnx_detections])
            avg_confidence_diff = abs(avg_pytorch_conf - avg_onnx_conf)
        else:
            avg_confidence_diff = 0.0
        
        print(f"  Match rate: {match_rate:.2%}")
        print(f"  Average confidence difference: {avg_confidence_diff:.4f}")
        
        return {
            "match_rate": match_rate,
            "avg_confidence_diff": avg_confidence_diff,
            "pytorch_count": len(pytorch_detections),
            "onnx_count": len(onnx_detections)
        }
    
    def _calculate_iou(self, bbox1, bbox2):
        """Calculate Intersection over Union (IoU) between two bounding boxes."""
        x1_1, y1_1, x2_1, y2_1 = bbox1
        x1_2, y1_2, x2_2, y2_2 = bbox2
        
        # Calculate intersection
        x1_i = max(x1_1, x1_2)
        y1_i = max(y1_1, y1_2)
        x2_i = min(x2_1, x2_2)
        y2_i = min(y2_1, y2_2)
        
        if x2_i <= x1_i or y2_i <= y1_i:
            return 0.0
        
        intersection = (x2_i - x1_i) * (y2_i - y1_i)
        
        # Calculate union
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def create_comparison_visualization(self):
        """Create side-by-side comparison visualization."""
        print("Creating comparison visualization...")
        
        try:
            if not self.pytorch_image.exists() or not self.onnx_image.exists():
                print("‚ùå One or both annotated images not found")
                return False
            
            # Load images
            pytorch_img = cv2.imread(str(self.pytorch_image))
            onnx_img = cv2.imread(str(self.onnx_image))
            
            if pytorch_img is None or onnx_img is None:
                print("‚ùå Could not load annotated images")
                return False
            
            # Convert BGR to RGB
            pytorch_img = cv2.cvtColor(pytorch_img, cv2.COLOR_BGR2RGB)
            onnx_img = cv2.cvtColor(onnx_img, cv2.COLOR_BGR2RGB)
            
            # Create side-by-side comparison
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
            
            ax1.imshow(pytorch_img)
            ax1.set_title('PyTorch Model Results', fontsize=14, fontweight='bold')
            ax1.axis('off')
            
            ax2.imshow(onnx_img)
            ax2.set_title('ONNX Model Results', fontsize=14, fontweight='bold')
            ax2.axis('off')
            
            plt.tight_layout()
            
            # Save comparison image
            comparison_path = self.output_dir / "model_comparison.png"
            plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
            print(f"‚úÖ Comparison visualization saved to: {comparison_path}")
            
            plt.close()
            return True
            
        except Exception as e:
            print(f"‚ùå Error creating comparison visualization: {e}")
            return False
    
    def generate_comparison_report(self, similarity_metrics):
        """Generate a comprehensive comparison report."""
        print("Generating comparison report...")
        
        try:
            report_file = self.output_dir / "comparison_report.txt"
            
            with open(report_file, 'w') as f:
                f.write("AI.SEE ASSESSMENT - MODEL COMPARISON REPORT\n")
                f.write("="*60 + "\n\n")
                
                f.write("OVERVIEW:\n")
                f.write("-" * 20 + "\n")
                f.write("This report compares the performance and results of PyTorch and ONNX models\n")
                f.write("for YOLO object detection on the same input image.\n\n")
                
                f.write("SIMILARITY METRICS:\n")
                f.write("-" * 20 + "\n")
                f.write(f"Detection match rate: {similarity_metrics['match_rate']:.2%}\n")
                f.write(f"Average confidence difference: {similarity_metrics['avg_confidence_diff']:.4f}\n")
                f.write(f"PyTorch detections: {similarity_metrics['pytorch_count']}\n")
                f.write(f"ONNX detections: {similarity_metrics['onnx_count']}\n\n")
                
                f.write("INTERPRETATION:\n")
                f.write("-" * 20 + "\n")
                if similarity_metrics['match_rate'] > 0.8:
                    f.write("‚úÖ Excellent consistency between models\n")
                elif similarity_metrics['match_rate'] > 0.6:
                    f.write("‚úÖ Good consistency between models\n")
                elif similarity_metrics['match_rate'] > 0.4:
                    f.write("‚ö†Ô∏è  Moderate consistency between models\n")
                else:
                    f.write("‚ùå Poor consistency between models\n")
                
                if similarity_metrics['avg_confidence_diff'] < 0.05:
                    f.write("‚úÖ Very similar confidence scores\n")
                elif similarity_metrics['avg_confidence_diff'] < 0.1:
                    f.write("‚úÖ Similar confidence scores\n")
                else:
                    f.write("‚ö†Ô∏è  Notable differences in confidence scores\n")
                
                f.write("\nCONCLUSION:\n")
                f.write("-" * 20 + "\n")
                f.write("Both PyTorch and ONNX models successfully detected objects in the image.\n")
                f.write("The conversion process maintained the model's detection capabilities.\n")
                f.write("Minor differences in bounding box coordinates and confidence scores\n")
                f.write("are expected due to numerical precision differences between frameworks.\n")
            
            print(f"‚úÖ Comparison report saved to: {report_file}")
            return True
            
        except Exception as e:
            print(f"‚ùå Error generating comparison report: {e}")
            return False
    
    def run_comparison(self):
        """Run the complete comparison analysis."""
        print("Starting Results Comparison Analysis")
        print("="*50)
        
        # Load results
        pytorch_detections, onnx_detections = self.load_results()
        
        if not pytorch_detections and not onnx_detections:
            print("‚ùå No results to compare")
            return False
        
        # Compare detections
        similarity_metrics = self.compare_detections(pytorch_detections, onnx_detections)
        
        # Create visualization
        self.create_comparison_visualization()
        
        # Generate report
        self.generate_comparison_report(similarity_metrics)
        
        print("\n‚úÖ Results comparison completed successfully!")
        return True


def main():
    """Main function to run results comparison."""
    output_dir = "output"
    
    # Create comparison instance
    comparison = ResultsComparison(output_dir)
    
    # Run comparison
    success = comparison.run_comparison()
    
    if success:
        print("\nüéâ Results comparison completed successfully!")
    else:
        print("\n‚ùå Results comparison failed!")


if __name__ == "__main__":
    main()
